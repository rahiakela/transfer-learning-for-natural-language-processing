{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2-linear-and-tree-based-models-for-movie-sentiment-classification.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOGZdmNDRbC3/sr/u9/F1wM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/transfer-learning-for-natural-language-processing/blob/main/2-getting-started-with-baselines/2_linear_and_tree_based_models_for_movie_sentiment_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0syenf2nUMgZ"
      },
      "source": [
        "# Linear & Tree-based models for Movie Sentiment Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Tvj1R7TUOC2"
      },
      "source": [
        "Our goal is to establish a set of baselines for a pair of concrete NLP problems, which we will later be able to use to measure progressive improvements gained from leveraging increasingly sophisticated transfer learning\r\n",
        "approaches. In the process of doing this, we aim to advance your general NLP instincts and refresh your understanding of typical procedures involved in setting up problem-solving pipelines for such problems. You will review techniques ranging from tokenization to data structure and model selection. We first train some traditional machine learning models from scratch to establish some preliminary baselines for these problems.\r\n",
        "\r\n",
        "We will focus on a pair of important representative example NLP problems – spam\r\n",
        "classification of email, and sentiment classification of movie reviews. This exercise will arm you with a number of important skills, including some tips for obtaining, visualizing and preprocessing data. \r\n",
        "\r\n",
        "Three major model classes will be covered, namely linear models such as logistic regression, decision-tree-based models such as random forests, and neural-network-based models such as ELMo. These classes are additionally represented by support vector machines (SVMs) with linear kernels, gradient-boosting machines (GBMs) and BERT respectively. \r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/transfer-learning-for-natural-language-processing/content-classification-supervised-models.png?raw=1' width='800'/>\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inPFIdfP7n4K"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xlt8No657pZ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5252b3e6-c7fd-40cd-d29c-e813deecaa69"
      },
      "source": [
        "import numpy as np  # linear algebra\r\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\r\n",
        "import email        # email package for processing email messages\r\n",
        "import random\r\n",
        "import re\r\n",
        "import os\r\n",
        "import time\r\n",
        "\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.svm import SVC                              # Support Vector Classification model\r\n",
        "from sklearn.ensemble import RandomForestClassifier      # random forest classifier library\r\n",
        "from sklearn.ensemble import GradientBoostingClassifier  # GBM algorithm\r\n",
        "\r\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score         # for tune parameters systematically\r\n",
        "from sklearn import metrics                              #Additional scklearn functions\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "\r\n",
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "from nltk.corpus import stopwords\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJKVZMbo7va7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bd01549-96de-44b7-cc17-9e62349bd349"
      },
      "source": [
        "%%shell\r\n",
        "\r\n",
        "wget -q \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\r\n",
        "tar xzf aclImdb_v1.tar.gz\r\n",
        "\r\n",
        "rm -rf aclImdb_v1.tar.gz\r\n",
        "rm -rf aclImdb/train/unsup"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQbAbRM7Wtfs"
      },
      "source": [
        "## Preprocessing Movie Sentiment Classification Example Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HplaHzeVWwfg"
      },
      "source": [
        "This notebook is concerned with classifying movie reviews from IMDB into\r\n",
        "positive or negative sentiments expressed. This is a prototypical sentiment analysis example that has been used widely in the literature to study many algorithms.\r\n",
        "\r\n",
        "We will use a popular labeled dataset of 25000 reviews for this, which was assembled by scraping the popular movie review website IMDB and mapping the number of stars corresponding to each review to either 0 or 1 – depending on whether it was less than or greater than 5 out of 10 stars respectively.It has been used widely in prior NLP literature, and this familiarity is part of the reason we choose it as an illustrative example for baselining.\r\n",
        "\r\n",
        "The sequence of steps used to preprocess each IMDB movie review before analysis is very similar to the one presented for the email spam classification example.\r\n",
        "\r\n",
        "The first major difference is that no email headers are attached to these reviews, so the header extraction step is not applicable. \r\n",
        "\r\n",
        "Additionally, since some stopwords – including “no” and “not” – may\r\n",
        "change the sentiment of the message, the stopword removal step may need to be carried out with extra care, first making sure to drop such stopwords from the target list. We did experiment with dropping such words from the list, and saw little to no effect on the result. This is likely because other non-stopwords in the reviews are very predictive features, rendering this step irrelevant.\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/transfer-learning-for-natural-language-processing/spam-email-preprocessing.png?raw=1' width='800'/>\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsXoqMRdwC65"
      },
      "source": [
        "### IMDB Movie Review Dataset preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeeYY2_OuxYw"
      },
      "source": [
        "Before proceeding, we must decide how many samples to draw from each class. We must also decide the maximum number of tokens per email, and the maximum length of each token. This is done by setting the following overarching hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_qzNMcNsGh-"
      },
      "source": [
        "n_sample = 1000   # number of samples to generate in each class\r\n",
        "maxtokens = 50    # the maximum number of tokens per document\r\n",
        "maxtokenlen = 20  # the maximum length of each token"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRVD-qopxT2f"
      },
      "source": [
        "With these hyperparameters specified, we can now create a single DataFrame for the overarching training dataset. Let’s take the opportunity to also perform remaining preprocessing tasks, namely removing stop words, punctuations and tokenizing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2OTWZJ6w9-J"
      },
      "source": [
        "#### Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-dJjXYvxaK9"
      },
      "source": [
        "Let’s proceed by defining a function to tokenize text by splitting them into \r\n",
        "words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42b-458pwdRh"
      },
      "source": [
        "def tokenize(row):\r\n",
        "  if row is None or row is \"\":\r\n",
        "    tokens = \"\"\r\n",
        "  else:\r\n",
        "    tokens = row.split(\" \")[:maxtokens]\r\n",
        "  return tokens"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_viulZEyUNO"
      },
      "source": [
        "#### Remove punctuation and unnecessary characters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MOkUJbxyUeG"
      },
      "source": [
        "**In order to ensure that classification is done based on language content only, we have to remove punctuation marks and other non-word characters from the emails.** We do this by employing regular expressions with the Python regex library. We also normalize words by turning them into lower case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2n-aNOVNx3mC"
      },
      "source": [
        "def reg_expressions(row):\r\n",
        "  tokens = []\r\n",
        "  try:\r\n",
        "    for token in row:\r\n",
        "      token = token.lower()          # make all characters lower case\r\n",
        "      token = re.sub(r\"[\\W\\d]\", \"\", token)\r\n",
        "      token = token[:maxtokenlen]    # truncate all tokens to hyperparameter maxtokenlen\r\n",
        "      tokens.append(token)\r\n",
        "  except:\r\n",
        "    token = \"\"\r\n",
        "    tokens.append(token)\r\n",
        "  return tokens"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ubr6_29dzvyA"
      },
      "source": [
        "#### Stop-word removal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjpNoFWhzwqu"
      },
      "source": [
        "Stop-words are also removed. Stop-words are words that are very common in text but offer no useful information that can be used to classify the text. Words such as is, and, the, are are examples of stop-words. The NLTK library contains a list of 127 English stop-words and can be used to filter our tokenized strings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fObPMgFUZ40",
        "outputId": "29491edf-c44e-4971-bfdb-c3cb4b437269",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "stop_words = stopwords.words(\"english\")\r\n",
        "print(stop_words)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoJ-_4jZUr0N"
      },
      "source": [
        "# it may be beneficial to drop negation words from the removal list, as they can change the positive/negative meaning of a sentence\r\n",
        "stop_words.remove(\"no\")\r\n",
        "stop_words.remove(\"nor\")\r\n",
        "stop_words.remove(\"not\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75fyCvyDzpFd"
      },
      "source": [
        "def stop_word_removal(row):\r\n",
        "  token = [token for token in row if token not in stop_words]\r\n",
        "  token = filter(None, token)\r\n",
        "\r\n",
        "  return token"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sj0jsZ9W65Yo"
      },
      "source": [
        "### Converting the Sentiment Text Into Numbers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALmmE6Wo7A-6"
      },
      "source": [
        "We start by employing what is often considered the simplest method for vectorizing words, i.e., converting them into numerical vectors – the bag-of-words model. This model simply counts the frequency of word tokens contained in each email and thereby represents it as a vector of such frequency counts.\r\n",
        "\r\n",
        "Please observe that in doing this, we only retain tokens that appear more than once, as captured by the variable “used_tokens”. This enables us to keep the vector dimensions significantly lower than they would be otherwise. Please also\r\n",
        "note that one can achieve this using various in-built vectorizers in the popular library scikitlearn.\r\n",
        "\r\n",
        "We also note the scikit-learn vectorization methods include counting occurrences of sequences of any n words, or n-grams, as well as the tf-idf approach – important fundamental concepts you should brush on if rusty. For the problems looked at here, we did not notice an improvement when using these vectorization methods over the bag-of words approach.\r\n",
        "\r\n",
        "For the computer to make inferences of the sentiment, it has to be able to interpret the text by making a numerical representation of it. One way to do this is by using something called a \"bag-of-words\" model. This model simply counts the frequency of word tokens for each sentiment and thereby represents it as a vector of these counts.\r\n",
        "\r\n",
        "The assemble_bag() function assembles a new dataframe containing all the unique words found in the text documents. It counts the word frequency and then returns the new dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ve_ovHA36tk2"
      },
      "source": [
        "def assemble_bag(data):\r\n",
        "  used_tokens = []\r\n",
        "  all_tokens = []\r\n",
        "\r\n",
        "  for item in data:\r\n",
        "    for token in item:\r\n",
        "      if token in all_tokens:\r\n",
        "        # If token has been seen before, append it to output list used_tokens\r\n",
        "        if token not in used_tokens:\r\n",
        "          used_tokens.append(token)\r\n",
        "      else:\r\n",
        "        all_tokens.append(token)\r\n",
        "\r\n",
        "  df = pd.DataFrame(0, index=np.arange(len(data)), columns=used_tokens)\r\n",
        "\r\n",
        "  # Create a Pandas DataFrame counting frequencies of vocabulary words – corresponding to columns, in each email – corresponding to rows\r\n",
        "  for i, item in enumerate(data):\r\n",
        "    for token in item:\r\n",
        "      if token in used_tokens:\r\n",
        "        df.iloc[i][token] += 1\r\n",
        "\r\n",
        "  return df"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9A9TQ-krYXH1"
      },
      "source": [
        "Having fully vectorized the dataset, we must remember that it is not shuffled with respect to classes, i.e., it contains Nsamp = 1000 spam emails followed by an equal number of nonspam emails. Depending on how this dataset is split, in our case by picking the first 70% for training and the remainder for testing, this could lead to a training set composed of spam only, which would obviously lead to failure. In order to create a randomized ordering of class samples in the dataset, we will need to shuffle the data in unison with the header/list of labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPOqephWYXu6"
      },
      "source": [
        "# shuffle raw data first\r\n",
        "def unison_shuffle_data(data, header):\r\n",
        "  p = np.random.permutation(len(header))\r\n",
        "  data = data[p]\r\n",
        "  header = np.asarray(header)[p]\r\n",
        "\r\n",
        "  return data, header"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4w91A4yXqcc"
      },
      "source": [
        "Let's load dataset into a Numpy array after tokenizing, removing stopwords and punctuations, and shuffling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHgrPX9XX1Rr"
      },
      "source": [
        "# load data in appropriate form\r\n",
        "def load_data(path):\r\n",
        "  data, sentiments = [], []\r\n",
        "  for folder, sentiment in ((\"neg\", 0), (\"pos\", 1)):\r\n",
        "    folder = os.path.join(path, folder)\r\n",
        "    for name in os.listdir(folder):\r\n",
        "      with open(os.path.join(folder, name), \"r\") as reader:\r\n",
        "        text = reader.read()\r\n",
        "      text = tokenize(text)            # tokenizing\r\n",
        "      text = stop_word_removal(text)   # removing stopwords and punctuations\r\n",
        "      text = reg_expressions(text)\r\n",
        "      data.append(text)\r\n",
        "      sentiments.append(sentiment)     # Track corresponding sentiment labels\r\n",
        "\r\n",
        "  # converting to Numpy array\r\n",
        "  data_np = np.array(data)\r\n",
        "  # shuffling\r\n",
        "  data, sentiments = unison_shuffle_data(data_np, sentiments)\r\n",
        "\r\n",
        "  return data, sentiments"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLpNdWmlaYjB",
        "outputId": "59910e26-5a44-4492-bebe-dd0d5a8d0c77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_path = os.path.join(\"aclImdb\", \"train\")\r\n",
        "test_path = os.path.join(\"aclImdb\", \"test\")\r\n",
        "\r\n",
        "raw_data, raw_header = load_data(train_path)\r\n",
        "print(raw_data.shape)\r\n",
        "print(len(raw_header))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000,)\n",
            "25000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  app.launch_new_instance()\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kzga3_25aXw3"
      },
      "source": [
        "Let's take n_sample*2 random entries of the loaded data for training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUWw0t6fdhZf"
      },
      "source": [
        "# Subsample required number of samples\r\n",
        "random_indices = np.random.choice(range(len(raw_header)), size=(n_sample * 2, ), replace=False)\r\n",
        "data_train = raw_data[random_indices]\r\n",
        "header = raw_header[random_indices]\r\n",
        "\r\n",
        "print(\"DEBUG::data_train::\")\r\n",
        "print(data_train[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htBzFn8Bgb7N"
      },
      "source": [
        "Before proceeding, we need to check the balance of the resulting data with regards to class. In general, we don’t want one of the labels to represent most of the dataset, unless that is the distribution expected in practice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADYCfHfsgegJ",
        "outputId": "6954f125-1a43-46d9-8892-5463d43187e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Display sentiments and their frequencies in the dataset, to ensure it is roughly balanced between classes\r\n",
        "unique_elements, counts_elements = np.unique(header, return_counts=True)\r\n",
        "print(\"Sentiments and their frequencies:\")\r\n",
        "print(unique_elements)\r\n",
        "print(counts_elements)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentiments and their frequencies:\n",
            "[0 1]\n",
            "[ 985 1015]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCJpbAgNUChs"
      },
      "source": [
        "We are now ready to convert these into numerical vectors!!\r\n",
        "\r\n",
        "Having satisfied ourselves that the data is roughly balanced between the two classes, with each class representing roughly half of the dataset, assemble and visualize the bag-of-words representation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPvtszzgTaz5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "outputId": "03a411df-a2f0-4614-ff25-f1ac5eba6cd6"
      },
      "source": [
        "# create bag-of-words model\r\n",
        "mixed_bag_reviews  = assemble_bag(data_train) \r\n",
        "\r\n",
        "# this is the list of words in our bag-of-words model\r\n",
        "predictors = [column for column in mixed_bag_reviews.columns]\r\n",
        "\r\n",
        "# expand default pandas display options to make emails more clearly visible when printed\r\n",
        "pd.set_option(\"display.max_colwidth\", 300)\r\n",
        "\r\n",
        "mixed_bag_reviews"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>i</th>\n",
              "      <th>the</th>\n",
              "      <th>good</th>\n",
              "      <th>jim</th>\n",
              "      <th>carrey</th>\n",
              "      <th>not</th>\n",
              "      <th>br</th>\n",
              "      <th>it</th>\n",
              "      <th>movie</th>\n",
              "      <th>film</th>\n",
              "      <th>task</th>\n",
              "      <th>time</th>\n",
              "      <th>thing</th>\n",
              "      <th>would</th>\n",
              "      <th></th>\n",
              "      <th>rating</th>\n",
              "      <th>holes</th>\n",
              "      <th>hard</th>\n",
              "      <th>also</th>\n",
              "      <th>one</th>\n",
              "      <th>in</th>\n",
              "      <th>actors</th>\n",
              "      <th>could</th>\n",
              "      <th>its</th>\n",
              "      <th>many</th>\n",
              "      <th>find</th>\n",
              "      <th>another</th>\n",
              "      <th>little</th>\n",
              "      <th>mystery</th>\n",
              "      <th>romance</th>\n",
              "      <th>made</th>\n",
              "      <th>but</th>\n",
              "      <th>ditched</th>\n",
              "      <th>this</th>\n",
              "      <th>worse</th>\n",
              "      <th>audience</th>\n",
              "      <th>bad</th>\n",
              "      <th>hindi</th>\n",
              "      <th>seen</th>\n",
              "      <th>and</th>\n",
              "      <th>...</th>\n",
              "      <th>tour</th>\n",
              "      <th>akshay</th>\n",
              "      <th>kareena</th>\n",
              "      <th>kapoor</th>\n",
              "      <th>bikinibr</th>\n",
              "      <th>whoa</th>\n",
              "      <th>oregon</th>\n",
              "      <th>olympic</th>\n",
              "      <th>mercifully</th>\n",
              "      <th>gymnast</th>\n",
              "      <th>gymnastic</th>\n",
              "      <th>minus</th>\n",
              "      <th>gifted</th>\n",
              "      <th>hamiltons</th>\n",
              "      <th>siblings</th>\n",
              "      <th>eldest</th>\n",
              "      <th>surrogate</th>\n",
              "      <th>parent</th>\n",
              "      <th>unsure</th>\n",
              "      <th>hello</th>\n",
              "      <th>stole</th>\n",
              "      <th>pounds</th>\n",
              "      <th>hint</th>\n",
              "      <th>persuade</th>\n",
              "      <th>mutants</th>\n",
              "      <th>range</th>\n",
              "      <th>effectively</th>\n",
              "      <th>levels</th>\n",
              "      <th>technique</th>\n",
              "      <th>hardhitting</th>\n",
              "      <th>further</th>\n",
              "      <th>planning</th>\n",
              "      <th>cliches</th>\n",
              "      <th>rave</th>\n",
              "      <th>endeavor</th>\n",
              "      <th>filmsbr</th>\n",
              "      <th>adaptations</th>\n",
              "      <th>terrorist</th>\n",
              "      <th>grieves</th>\n",
              "      <th>deaths</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 5096 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      i  the  good  jim  ...  adaptations  terrorist  grieves  deaths\n",
              "0     2    0     1    0  ...            0          0        0       0\n",
              "1     0    2     0    0  ...            0          0        0       0\n",
              "2     1    0     1    4  ...            0          0        0       0\n",
              "3     0    0     0    0  ...            0          0        0       0\n",
              "4     1    1     0    0  ...            0          0        0       0\n",
              "...  ..  ...   ...  ...  ...          ...        ...      ...     ...\n",
              "1995  3    2     0    0  ...            0          0        0       0\n",
              "1996  0    0     0    0  ...            0          0        0       0\n",
              "1997  2    1     1    0  ...            0          0        0       0\n",
              "1998  3    0     0    0  ...            1          0        0       0\n",
              "1999  0    1     0    0  ...            0          1        1       1\n",
              "\n",
              "[2000 rows x 5096 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etyyQh-ZWWXf"
      },
      "source": [
        "With this numerical representation ready, we now proceed to building out our baseline classifiers in the subsequent sections for the two presented example datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIGDs4kZYCIr"
      },
      "source": [
        "As the very last step of preparing the sentiment dataset for training by our baseline classifiers, we split it into independent training and testing or validation sets. This will allow us to evaluate the performance of the classifier on a set of data that was not used for training, an important thing\r\n",
        "to ensure in machine learning practice. We elect to use 70% of the data for training, and 30% for testing/validation afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2bJ0TlTX3FW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10a142aa-4645-47d7-c350-1e7f745d436f"
      },
      "source": [
        "# split into independent 70% training and 30% testing sets\r\n",
        "data = mixed_bag_reviews.values\r\n",
        "\r\n",
        "idx = int(0.7 * data.shape[0])  # get 70% index value\r\n",
        "\r\n",
        "# 70% of data for training\r\n",
        "train_x = data[:idx, :]\r\n",
        "train_y = header[:idx]\r\n",
        "\r\n",
        "# remaining 30% for testing\r\n",
        "test_x = data[idx:, :]\r\n",
        "test_y = header[idx:]\r\n",
        "\r\n",
        "print(\"train_x/train_y list details, to make sure they are of the right form:\")\r\n",
        "print(len(train_x))\r\n",
        "print(train_x)\r\n",
        "print(len(train_y))\r\n",
        "print(train_y[:5])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_x/train_y list details, to make sure they are of the right form:\n",
            "1400\n",
            "[[2 0 1 ... 0 0 0]\n",
            " [0 2 0 ... 0 0 0]\n",
            " [1 0 1 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 1 0 ... 0 0 0]\n",
            " [2 2 0 ... 0 0 0]]\n",
            "1400\n",
            "[0 0 1 1 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HQRPMa4Z8RB"
      },
      "source": [
        "Since 70% of 2000 is 1400, looks good! (for n_sample=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRlxXUoKkD5x"
      },
      "source": [
        "### How about other vectorization strategies?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxAWeJ3MkFW-"
      },
      "source": [
        "Let's check some other vectorization strategies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnOrtEeOkR8P"
      },
      "source": [
        "# create the transform - uncomment the one you want to focus on\r\n",
        "# vectorizer = CountVectorizer()    # this is equivalent to the bag of words\r\n",
        "vectorizer = TfidfVectorizer()    # tf-idf vectorizer\r\n",
        "# vectorizer = HashingVectorizer(n_features=3000)  # hashing vectorizer"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdtXJQzTkzPS",
        "outputId": "5e6fef62-64e0-4f5a-aee7-717bb6390f7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# build vocabulary\r\n",
        "vectorizer.fit([\" \".join(sublist) for sublist in data_train])\r\n",
        "# summarize\r\n",
        "print(len(vectorizer.vocabulary_))\r\n",
        "print(vectorizer.idf_)\r\n",
        "\r\n",
        "# encode one document\r\n",
        "vector = vectorizer.transform([\" \".join(data_train[0])])\r\n",
        "# summarize encoded vector\r\n",
        "print(vector.shape)\r\n",
        "print(vector.toarray())\r\n",
        "\r\n",
        "# set this to 'True' if you want to use the vectorizer featurizers instead of the bag-of-words done before\r\n",
        "USE = True\r\n",
        "if USE:\r\n",
        "  data = vectorizer.transform([\" \".join(sublist) for sublist in data_train]).toarray()\r\n",
        "  # 70% of data for training\r\n",
        "  train_x = data[:idx, :]\r\n",
        "  # remaining 30% for testing\r\n",
        "  test_x = data[idx, :]\r\n",
        "\r\n",
        "  print(\"train_x/train_y list details, to make sure it is of the right form:\")\r\n",
        "  print(train_x.shape[0])\r\n",
        "  print(train_x)\r\n",
        "  print(train_y[:5])\r\n",
        "  print(len(train_y))\r\n",
        "  predictors = [column for column in vectorizer.vocabulary_]"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12095\n",
            "[7.90825515 7.90825515 7.90825515 ... 7.90825515 7.50279005 7.90825515]\n",
            "(1, 12095)\n",
            "[[0. 0. 0. ... 0. 0. 0.]]\n",
            "train_x/train_y list details, to make sure it is of the right form:\n",
            "1400\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "[0 0 1 1 0]\n",
            "1400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAdf-aVkadDa"
      },
      "source": [
        "## Generalized Linear Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyHf2bhnaeI8"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUorbYbDZw8b"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}